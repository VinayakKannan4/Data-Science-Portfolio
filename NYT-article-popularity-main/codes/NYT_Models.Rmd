---
title: "NYT Models"
author: "Olivier Kuhn de Chizelle"
date: "4/25/2022"
output: 
  html_document:
    theme: sandstone
    toc: TRUE
    toc_float: 
      collapsed: FALSE
      smooth_scroll: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Load Packages, include = FALSE}
library(randomForest) #for random forests
library(caret) # for CV folds and data splitting
library(ROCR) # for diagnostics and ROC plots/stats
library(pROC) # same as ROCR
library(stepPlr) # Firth;s logit implemented thru caret library
library(doParallel) # for using multiple processor cores
library(skimr)
library(gt)
library(corrplot)
library(GGally)
library(cvms)
library(tibble)
library(dplyr)
library(stats)
```
# Introduction
## Data

After generating features, the data is now complete and nearly read to be modeled. Let's make sure that each column is in the right data type in order to run models! 

### Load Data

```{r Load and Prepare Data}

data = read.csv('../data/fullDF.csv')

# Note that ~40% of the articles have >= 150 comments. This is our threshold for popularity
data = data %>% mutate(is_popular = case_when(
  n_comments < 150 ~ 'No',
  n_comments >= 150 ~ 'Yes'))

# print(table(data$is_popular))

```
### Subset columns 

```{r Subsetting}

df = data %>% subset(select = c(newsdesk, section, material, word_count, num_keywords, hour, month, day, week_count, is_weekend, contains_question, negative, neutral, positive, compound, is_racial, is_political, is_popular))



```

### Data Processing

### to Factor

```{r to factor}
# df$is_popular = factor(df$is_popular, levels = c(0,1), labels = c('No','Yes'))
# df$contains_question = factor(df$contains_question, levels = c(0,1), labels = c('No','Yes'))
# df$is_weekend = factor(df$is_weekend, levels = c(0,1), labels = c('No','Yes'))
# df$is_racial = factor(df$is_racial, levels = c(0,1), labels = c('No','Yes'))
# df$is_political = factor(df$is_political, levels = c(0,1), labels = c('No','Yes'))
# 
# for(i in 15:ncol(df)){
#   column = colnames(df)[i]
#   df[column] = factor(df[,column], levels = c(0,1), labels = c('No','Yes'))
# }

# ncol(df)
# colnames(df)[15]
# column = colnames(df)[16]
# df[column] = factor(df[column], levels = c(0,1), labels = c('No','Yes'))
# 
# 
# df$section_At.Home = factor(df$section_At.Home, levels = c(0,1), labels = c('No','Yes'))
# colnames(df)
# 
# df$section = factor(df$section)
# df$material = factor(df$material)
# df$is_weekend = factor(df$is_weekend, levels = c(0,1), labels = c('No','Yes'))
# df$contains_question = factor(df$contains_question)
# #df$overall_sentiment = factor(df$overall_sentiment)
# df$is_racial = factor(df$is_racial, levels = c(0,1), labels = c('No','Yes'))
# df$is_political = factor(df$is_political, levels = c(0,1), labels = c('No','Yes'))
# df$is_popular = factor(df$is_popular)


df$newsdesk = factor(df$newsdesk)
df$section = factor(df$section)
df$material = factor(df$material)
df$is_weekend = factor(df$is_weekend, levels = c(0,1), labels = c('No','Yes'))
df$contains_question = factor(df$contains_question)
df$is_racial = factor(df$is_racial, levels = c(0,1), labels = c('No','Yes'))
df$is_political = factor(df$is_political, levels = c(0,1), labels = c('No','Yes'))
df$is_popular = factor(df$is_popular)

# df$newsdesk = factor(df$newsdesk, levels = 0:(length(unique(df$newsdesk))-1), labels = unique(df$newsdesk))
# Sambanis$warstds = factor(Sambanis$warstds, levels = c(0,1), labels = c('peace','war'))
```

# Models

Now that we have completed data preparation, let's begin modeling.

## Logistic Regression

```{r logReg}



```

## Linear Discriminant Analysis

```{r LDA}
library(MASS)


# fit model
mod.LDA <- lda(is_popular~., family=multinomial, data=df)
# summarize the fit
summary(mod.LDA)
# make predictions
predictions = predict(mod.LDA, df[,1:ncol(df)-1])$class
# predictions = predict(mod.LDA, df[,1:ncol(df)-1])$class
# summarize accuracy
LDA.pred = ROCR::prediction(as.numeric(predictions), as.numeric(df$is_popular))
LDA.perf = performance(LDA.pred, 'tpr','fpr')
LDA.res = data.frame(table(predictions, df$is_popular))
LDA.accuracy = sum(LDA.res[LDA.res$predictions == LDA.res$Var2,]$Freq) / sum(LDA.res$Freq)

plot(LDA.perf)

LDAcmPlot = ggplot(LDA.res, aes(x = Var2, y= predictions, fill = Freq)) +
        geom_tile() + geom_text(aes(label=scales::percent(Freq/sum(Freq)))) +
        scale_fill_gradient(low="#f7f7f7", high="#beaed4") +
        labs(x = "Reference",y = "Prediction") +
        scale_x_discrete(labels=c("No","Yes")) +
        scale_y_discrete(labels=c("No","Yes")) +
        theme_minimal() +
        labs(x = 'Actual',
             y = 'Predicted',
             title = paste0("**NYT article popularity prediction — LDA**")) +
        theme(plot.title = ggtext::element_markdown(lineheight = 1.1, hjust = 0.5),
              plot.caption = ggtext::element_markdown(lineheight = 1.1),
              legend.text = ggtext::element_markdown(size = 11),
              axis.text.y = element_text(size = 8, angle = 0),
              axis.text.x = element_text(size = 11),
              axis.title.y = element_text(size = 14),
              axis.title.x = element_text(size = 12)) + 
        guides(fill = FALSE, size = FALSE)
LDAcmPlot
ggsave('../images/LDA_cm.png', plot = LDAcmPlot)


#Get precision and recall
t = table(predictions, df$is_popular)
p = (t[2,2] / (t[2,2] + t[1,2]))
r = t[1,1] / (t[1,1] + t[1,2])
LDA.precision = round(p ,2)
LDA.recall = round(r, 2)
LDA.f1 = (2*p*r) / (p + r)
LDA.f1= round(LDA.f1, 2)


```

## Decision Tree

```{r DTree}


```

## AdaBoost

```{r AdaBoost}
# 5 fold Cross Validation Train Control
# tc5<-trainControl(method="cv", 
#                  number=5, #creates CV folds - 5 for this data
#                  summaryFunction=twoClassSummary, # provides ROC summary stats in call to model
#                  savePredictions = TRUE,
#                  returnData = TRUE,
#                  classProbs = TRUE)
# 
# mod.ab5 = train(as.factor(is_popular) ~ ., metric = 'ROC', method = 'adaboost', trControl = tc5, data = df)
# 
# ab5.pred = ROCR::prediction(as.numeric(mod.ab5$pred$pred), as.numeric(mod.ab5$pred$obs))
# perf.ab5 = performance(ab5.pred, 'tpr','fpr')
# ab.accuracy5 = sum(mod.ab5$pred$pred == mod.ab5$pred$obs) / length(mod.ab5$pred$pred)

```

## Gradient Boost

```{r gradientBoost}

tc5<-trainControl(method="cv", 
                 number=5, #creates CV folds - 5 for this data
                 summaryFunction=twoClassSummary, # provides ROC summary stats in call to model
                 savePredictions = TRUE,
                 returnData = TRUE,
                 classProbs = TRUE)
#5-FOLD CV
#mod.gb5 = train(is_popular ~ ., metric = 'ROC', method = 'gbm', trControl = tc5, data = df)
mod.gb5 = train(as.factor(is_popular) ~ ., metric = 'ROC', method = 'gbm', trControl = tc5, data = df)

gb5.pred = ROCR::prediction(as.numeric(mod.gb5$pred$pred), as.numeric(mod.gb5$pred$obs))
perf.gb5 = performance(gb5.pred, 'tpr','fpr')
gb.accuracy5 = sum(mod.gb5$pred$pred == mod.gb5$pred$obs) / length(mod.gb5$pred$pred)

gb5.res = data.frame(table(mod.gb5$pred$pred,mod.gb5$pred$obs))

GBcmPlot = ggplot(gb5.res, aes(x = Var2, y= Var1, fill = Freq)) +
        geom_tile() + geom_text(aes(label=scales::percent(Freq/sum(Freq)))) +
        scale_fill_gradient(low="#f7f7f7", high="#beaed4") +
        labs(x = "Reference",y = "Prediction") +
        scale_x_discrete(labels=c("No","Yes")) +
        scale_y_discrete(labels=c("No","Yes")) +
        theme_minimal() +
        labs(x = 'Actual',
             y = 'Predicted',
             title = paste0("**NYT article popularity prediction — Gradient Boost**")) +
        theme(plot.title = ggtext::element_markdown(lineheight = 1.1, hjust = 0.5),
              plot.caption = ggtext::element_markdown(lineheight = 1.1),
              legend.text = ggtext::element_markdown(size = 11),
              axis.text.y = element_text(size = 8, angle = 0),
              axis.text.x = element_text(size = 11),
              axis.title.y = element_text(size = 14),
              axis.title.x = element_text(size = 12)) + 
        guides(fill = FALSE, size = FALSE)
GBcmPlot
ggsave('../images/GB_cm.png', plot = GBcmPlot)

```



## Random Forest

```{r RandForest}
# 
# tc2<-trainControl(method="cv", 
#                  number=2, #creates CV folds - 5 for this data
#                  summaryFunction=twoClassSummary, # provides ROC summary stats in call to model
#                  savePredictions = TRUE,
#                  returnData = TRUE,
#                  classProbs = TRUE)


mod.rf5 = train(as.factor(is_popular) ~ .,
                  metric = 'ROC', 
                  method = 'rf', 
                  trControl = tc5, 
                  data = df, 
                  importance = T, 
                  proximity = T, 
                  ntree = 20)

# mod.rf5 = train(as.factor(is_popular) ~ .,
#                   metric = 'ROC', 
#                   method = 'rf', 
#                   trControl = tc2, 
#                   data = df, 
#                   importance = T, 
#                   proximity = T, 
#                   ntree = 100)

rf5.pred = ROCR::prediction(as.numeric(mod.rf5$pred$pred), as.numeric(mod.rf5$pred$obs))
perf.rf5 = performance(rf5.pred, 'tpr','fpr')
rf.accuracy5 = sum(mod.rf5$pred$pred == mod.rf5$pred$obs) / length(mod.rf5$pred$pred)

rf5.predictions = predict(mod.rf5, df[,1:ncol(df)-1])
rf5.res = data.frame(table(mod.rf5$finalModel$predicted,df$is_popular))
plot(perf.rf5)

sum(mod.rf5$finalModel$predicted == df$is_popular) / nrow(df)

RFcmPlot = ggplot(rf5.res, aes(x = Var2, y= Var1, fill = Freq)) +
        geom_tile() + geom_text(aes(label=scales::percent(Freq/sum(Freq)))) +
        scale_fill_gradient(low="#f7f7f7", high="#beaed4") +
        labs(x = "Reference",y = "Prediction") +
        scale_x_discrete(labels=c("No","Yes")) +
        scale_y_discrete(labels=c("No","Yes")) +
        theme_minimal() +
        labs(x = 'Actual',
             y = 'Predicted',
             title = paste0("**NYT article popularity prediction — Random Forest**")) +
        theme(plot.title = ggtext::element_markdown(lineheight = 1.1, hjust = 0.5),
              plot.caption = ggtext::element_markdown(lineheight = 1.1),
              legend.text = ggtext::element_markdown(size = 11),
              axis.text.y = element_text(size = 8, angle = 0),
              axis.text.x = element_text(size = 11),
              axis.title.y = element_text(size = 14),
              axis.title.x = element_text(size = 12)) + 
        guides(fill = FALSE, size = FALSE)

ggsave('../images/RF_cm.png', plot = RFcmPlot)



#Get precision and Recall
t = table(mod.rf5$finalModel$predicted,df$is_popular)
p = (t[2,2] / (t[2,2] + t[1,2]))
r = t[1,1] / (t[1,1] + t[1,2])
RF.precision = round(p ,2)
RF.recall = round(r, 2)
RF.f1 = (2*p*r) / (p + r)
RF.f1= round(RF.f1, 2)


```


```{r important variables}

imp_var = data.frame(mod.rf5$finalModel$importance)
imp_var = imp_var[order(-imp_var$MeanDecreaseGini),]

top10_variable <- head(imp_var, 10)
top10_variable$variables <- rownames(top10_variable)

top10_plot <- ggplot(data = top10_variable) + 
    geom_bar(mapping = aes(y = reorder(variables,
                                      MeanDecreaseGini),
                         x = MeanDecreaseGini),
           stat = "identity",
           position = "dodge",
           fill = "#beaed4") + 
    theme_classic() +
    labs(x = "Gini", 
         y = "Variables",
         title = paste0("**What are the top *10 most important variables*?**  
                        <span style='font-size:11pt'>By 
                        <span style='color:#beaed4;'>Mean Decrease in Gini</span>
                        </span>")) +
    theme(plot.title = ggtext::element_markdown(lineheight = 1.1, hjust = 0.5),
                plot.caption = ggtext::element_markdown(lineheight = 1.1),
                legend.text = ggtext::element_markdown(size = 11),
                axis.text.y = element_text(size = 8, angle = 0),
                axis.text.x = element_text(size = 8),
                axis.title.y = element_text(size = 14),
                axis.title.x = element_text(size = 12)) + 
    guides(fill = FALSE, size = FALSE)


ggsave('../images/top10varImp.png', plot = top10_plot)
top10_plot

```

```{r Logistic Regression and DT confusion matrix Plot from Avi}
#Logistic Regression
Var1 = c('No','Yes','No','Yes')
Var2 = c('No','No','Yes','Yes')
Freq = c(1760,258,471,869)
Log.res = data.frame(Var1, Var2, Freq)


LogcmPlot = ggplot(Log.res, aes(x = Var2, y= Var1, fill = Freq)) +
        geom_tile() + geom_text(aes(label=scales::percent(Freq/sum(Freq)))) +
        scale_fill_gradient(low="#f7f7f7", high="#beaed4") +
        labs(x = "Reference",y = "Prediction") +
        scale_x_discrete(labels=c("No","Yes")) +
        scale_y_discrete(labels=c("No","Yes")) +
        theme_minimal() +
        labs(x = 'Actual',
             y = 'Predicted',
             title = paste0("**NYT article popularity prediction — Logistic Regression**")) +
        theme(plot.title = ggtext::element_markdown(lineheight = 1.1, hjust = 0.5),
              plot.caption = ggtext::element_markdown(lineheight = 1.1),
              legend.text = ggtext::element_markdown(size = 11),
              axis.text.y = element_text(size = 8, angle = 0),
              axis.text.x = element_text(size = 11),
              axis.title.y = element_text(size = 14),
              axis.title.x = element_text(size = 12)) + 
        guides(fill = FALSE, size = FALSE)

LogcmPlot
ggsave('../images/Log_cm.png', plot = RFcmPlot)

#Decision Tree
Var1 = c('No','Yes','No','Yes')
Var2 = c('No','No','Yes','Yes')
Freq= c(1778,240,495,845)
DT.res = data.frame(Var1, Var2, Freq)


DTcmPlot = ggplot(DT.res, aes(x = Var2, y= Var1, fill = Freq)) +
        geom_tile() + geom_text(aes(label=scales::percent(Freq/sum(Freq)))) +
        scale_fill_gradient(low="#f7f7f7", high="#beaed4") +
        labs(x = "Reference",y = "Prediction") +
        scale_x_discrete(labels=c("No","Yes")) +
        scale_y_discrete(labels=c("No","Yes")) +
        theme_minimal() +
        labs(x = 'Actual',
             y = 'Predicted',
             title = paste0("**NYT article popularity prediction — Decision Tree**")) +
        theme(plot.title = ggtext::element_markdown(lineheight = 1.1, hjust = 0.5),
              plot.caption = ggtext::element_markdown(lineheight = 1.1),
              legend.text = ggtext::element_markdown(size = 11),
              axis.text.y = element_text(size = 8, angle = 0),
              axis.text.x = element_text(size = 11),
              axis.title.y = element_text(size = 14),
              axis.title.x = element_text(size = 12)) + 
        guides(fill = FALSE, size = FALSE)

DTcmPlot
ggsave('../images/DT_cm.png', plot = RFcmPlot)

# Random Forest from AVi
#Decision Tree
Var1 = c('No','Yes','No','Yes')
Var2 = c('No','No','Yes','Yes')
Freq= c(1820,198,493,847)
RF.avi = data.frame(Var1, Var2, Freq)


AviRFcmPlot = ggplot(RF.avi, aes(x = Var2, y= Var1, fill = Freq)) +
        geom_tile() + geom_text(aes(label=scales::percent(Freq/sum(Freq)))) +
        scale_fill_gradient(low="#f7f7f7", high="#beaed4") +
        labs(x = "Reference",y = "Prediction") +
        scale_x_discrete(labels=c("No","Yes")) +
        scale_y_discrete(labels=c("No","Yes")) +
        theme_minimal() +
        labs(x = 'Actual',
             y = 'Predicted',
             title = paste0("**NYT article popularity prediction — Random Forest**")) +
        theme(plot.title = ggtext::element_markdown(lineheight = 1.1, hjust = 0.5),
              plot.caption = ggtext::element_markdown(lineheight = 1.1),
              legend.text = ggtext::element_markdown(size = 11),
              axis.text.y = element_text(size = 8, angle = 0),
              axis.text.x = element_text(size = 11),
              axis.title.y = element_text(size = 14),
              axis.title.x = element_text(size = 12)) + 
        guides(fill = FALSE, size = FALSE)

AviRFcmPlot
ggsave('../images/AviRF_cm.png', plot = RFcmPlot)

```


